# ğŸ§  RAG Corp â€” Robust PDF & GitHub Retrieval-Augmented Generation System  

![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)
![LangChain](https://img.shields.io/badge/LangChain-Framework-green)
![Ollama](https://img.shields.io/badge/Ollama-Local--LLM-orange)
![FAISS](https://img.shields.io/badge/FAISS-Vector%20Search-yellow)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

> ğŸš€ A modular, **production-grade Retrieval-Augmented Generation (RAG)** system with **PDF & GitHub ingestion**, **FAISS vector search**, and **local LLM inference** through Ollama â€” designed for real-world, offline AI pipelines.

---

## ğŸ—ï¸ Overview  

**RAG Corp** is a corporate-style RAG pipeline designed to answer questions from **PDF documents** and **GitHub repositories** using local LLMs.  
Itâ€™s built for **offline, secure, and configurable deployments**, with robust logging, modular architecture, and full error handling.

---

## âœ¨ Features  

### ğŸ—‚ï¸ Multi-Source Ingestion  
- **PDF Loader** â€“ Extracts text from uploaded PDFs  
- **GitHub Repo Loader** â€“ Clones repos and indexes documentation/code  
- **Configurable via YAML** â€“ Control chunk size, model name, retriever depth, etc.  

### ğŸ§® Vector Store with FAISS  
- Fast, memory-efficient similarity search  
- Persistent offline index in `data/index/faiss/`  
- Incremental updates supported  

### ğŸ¤– Local LLM + Embeddings  
- **Ollama-powered models** (`llama3.2:1b`, `tinyllama:1.1b`, `deepseek-coder:6.7b`)  
- **Embeddings:** `nomic-embed-text` or fallback to `bge-m3`  
- Fully offline pipeline â€” no API calls or cloud dependency  

### ğŸ§° Robust Architecture  
- Centralized logging with `logging_conf.py`  
- Error-handled RAG core (graceful model or data fallback)  
- Modular structure for easy extension (Chroma, Weaviate, etc.)  
- Auto-discovery of configuration paths  

### ğŸ’» Streamlit UI  
- Clean, responsive dashboard  
- Upload PDFs or Git repos directly  
- Real-time logs: document count, index progress, error reports  
- Query chat interface for retrieval-based QA  

---

## ğŸ§± Project Structure  

Rag_application/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/ # (Optional) REST endpoints (FastAPI-ready)
â”‚ â”œâ”€â”€ configs/ # YAML configs
â”‚ â”œâ”€â”€ rag-corp/
â”‚ â”‚ â”œâ”€â”€ rag_core/
â”‚ â”‚ â”‚ â”œâ”€â”€ embeddings/ # Ollama + HF embedding providers
â”‚ â”‚ â”‚ â”œâ”€â”€ llm/ # Local LLM interface (Ollama)
â”‚ â”‚ â”‚ â”œâ”€â”€ loaders/ # GitHub + PDF data ingestion
â”‚ â”‚ â”‚ â”œâ”€â”€ vectorstores/ # FAISS integration
â”‚ â”‚ â”‚ â”œâ”€â”€ config.py # Auto-path + .env aware settings loader
â”‚ â”‚ â”‚ â”œâ”€â”€ logging_conf.py # Central logging system
â”‚ â”‚ â”‚ â”œâ”€â”€ rag_service.py # RAG logic orchestration
â”‚ â”‚ â”‚ â””â”€â”€ utils.py # Helpers / validation
â”‚ â””â”€â”€ ui/ # Streamlit front-end
â”œâ”€â”€ configs/settings.yaml # Global configuration
â”œâ”€â”€ data/ # Local index storage
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env.example



---

## âš™ï¸ Tech Stack  

| Layer | Technology | Purpose |
|:------|:------------|:---------|
| UI | Streamlit | Web-based dashboard |
| Core Framework | LangChain | RAG orchestration |
| LLM | Ollama | Local inference engine |
| Vector Database | FAISS | Embedding similarity search |
| Config | YAML + dotenv | Dynamic environment setup |
| Deployment | Docker + Compose | Reproducible environments |

---

## ğŸ§  How It Works  

1. **Upload PDF / GitHub URL**  
2. **Text Chunking & Embedding**  
   - Split into overlapping chunks  
   - Generate embeddings (Ollama / HF)  
3. **Vector Indexing**  
   - Stored in FAISS index for fast retrieval  
4. **User Query â†’ Retrieval + LLM Answer**  
   - Relevant chunks retrieved  
   - LLM synthesizes answer with context  

---

## ğŸ§¾ Configuration  

All runtime settings are stored in `configs/settings.yaml`:

`yaml
env: dev
data_dir: data
index_dir: data/index/faiss
chunk_size: 1000
chunk_overlap: 200
retriever_k: 4
embed_model: bge-m3
llm_model: llama3.2:1b
git:
  branch: main
  include_exts: [".py", ".md", ".txt"]
  exclude_dirs: ["__pycache__", "tests"]

export CONFIG_PATH=./configs/settings.yaml

---


## Environment setup:

git clone https://github.com/<your-username>/Rag_application.git
cd Rag_application

---

## Create environment
python3 -m venv .venv
source .venv/bin/activate
---

## Install dependencies
pip install -r requirements.txt
---

## Copy env template
cp .env.example .env
---

## Run
streamlit run app/ui/app.py
---

## Ollama services

ollama serve

ollama pull llama3.2:1b
ollama pull tinyllama:1.1b
ollama pull deepseek-coder:6.7b-instruct
ollama pull nomic-embed-text
---

## UI

<img width="1912" height="947" alt="Screenshot 2025-11-04 at 8 09 59â€¯PM" src="https://github.com/user-attachments/assets/c48e177d-efda-4ef2-ad93-3ef07045d3a4" />

---


